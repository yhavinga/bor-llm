import glob
import json

import numpy as np
import pandas as pd

MODEL_PARAMS = {
    "gpt-neo-125M-dutch": 176_000_000,
    "gpt2-medium-dutch": 380_000_000,
    "gpt2-large-dutch": 812_000_000,
    "Bor-1B": 1_190_000_000,
    "Llama-3.2-1B": 1_240_000_000,
    "gpt-neo-1.3B-dutch": 1_420_000_000,
    "SmolLM2-1.7B": 1_710_000_000,
    "Fietje-2": 2_700_000_000,
    "Phi-3.5-mini-instruct": 3_800_000_000,
}

MODEL_LANGUAGE = {
    "gpt-neo-125M-dutch": "ğŸ‡³ğŸ‡±",  # Dutch only
    "gpt2-medium-dutch": "ğŸ‡³ğŸ‡±",  # Dutch only
    "gpt2-large-dutch": "ğŸ‡³ğŸ‡±",  # Dutch only
    "gpt-neo-1.3B-dutch": "ğŸ‡³ğŸ‡±",  # Dutch only
    "Bor-1B": "ğŸ‡³ğŸ‡±ğŸ‡¬ğŸ‡§",  # Dutch + English
    "Llama-3.2-1B": "ğŸŒ",  # Multilingual
    "SmolLM2-1.7B": "ğŸ‡¬ğŸ‡§",  # English
    "Fietje-2": "ğŸ‡³ğŸ‡±*",  # Dutch-tuned Phi
    "Phi-3.5-mini-instruct": "ğŸŒ",  # Multilingual
}

MODEL_CONTEXT_LENGTH = {
    "gpt-neo-125M-dutch": 512,
    "gpt2-medium-dutch": 512,
    "gpt2-large-dutch": 512,
    "gpt-neo-1.3B-dutch": 512,
    "Bor-1B": 4096,
    "Llama-3.2-1B": 128_000,
    "SmolLM2-1.7B": 8192,
    "Fietje-2": 2048,
    "Phi-3.5-mini-instruct": 128_000,
}


def load_results(filename):
    with open(filename, "r") as f:
        return json.load(f)


def compute_averages():
    results = []

    # Define dataset order
    dataset_order = ["mc4_nl", "culturax_nl", "fineweb_edu_4"]

    for filename in glob.glob("outputs/*_evaluation_results.json"):
        model_name = filename.replace("outputs/", "").replace(
            "_evaluation_results.json", ""
        )
        data = load_results(filename)

        for dataset_name, dataset_data in data.items():
            if dataset_name not in dataset_order:
                continue

            if (
                not isinstance(dataset_data, dict)
                or "text_level_metrics" not in dataset_data
            ):
                continue

            # Calculate average tokens per word across all texts
            total_tokens = sum(
                text["num_tokens"] for text in dataset_data["text_level_metrics"]
            )
            total_words = sum(
                text["num_words"] for text in dataset_data["text_level_metrics"]
            )

            results.append(
                {
                    "model": model_name,
                    "dataset": dataset_name,
                    "token_perplexity": dataset_data["aggregated_metrics"][
                        "mean_perplexity"
                    ],
                    "bits_per_word": dataset_data["aggregated_metrics"][
                        "mean_bits_per_word"
                    ],
                    "tokens_per_word": total_tokens / total_words,
                }
            )

    df = pd.DataFrame(results)
    df = df.round(2)

    # Print model parameters table first
    print("\n### Model Parameters\n")
    params_df = pd.DataFrame([MODEL_PARAMS]).T.reset_index()
    params_df.columns = ["Model", "Parameters"]
    params_df["Language"] = params_df["Model"].map(MODEL_LANGUAGE)
    params_df["Context Length"] = params_df["Model"].map(MODEL_CONTEXT_LENGTH)
    params_df = params_df.sort_values("Parameters")
    print(params_df.to_markdown(index=False))

    # Create pivot tables with correct index (datasets only)
    metrics = ["token_perplexity", "bits_per_word", "tokens_per_word"]
    metric_names = {
        "token_perplexity": "Token Perplexity",
        "bits_per_word": "Bits per Word",
        "tokens_per_word": "Tokens per Word",
    }

    for metric in metrics:
        print(f"\n### {metric_names[metric]}\n")
        pivot_df = df.pivot(index="dataset", columns="model", values=metric)

        # Sort columns by model size
        model_sizes = {col: MODEL_PARAMS.get(col, 0) for col in pivot_df.columns}
        sorted_columns = sorted(pivot_df.columns, key=lambda x: model_sizes[x])
        pivot_df = pivot_df[sorted_columns]
        pivot_df = pivot_df.reindex(dataset_order)

        # Rename index for better readability
        index_names = {
            "mc4_nl": "MC4 (Dutch)",
            "culturax_nl": "CulturaX (Dutch)",
            "fineweb_edu_4": "Fineweb (English)",
        }
        pivot_df.index = pivot_df.index.map(index_names)

        print(pivot_df.round(2).to_markdown())


if __name__ == "__main__":
    compute_averages()

"""
### Model Parameters

| Model                 |   Parameters | Language   |   Context Length |
|:----------------------|-------------:|:-----------|-----------------:|
| gpt-neo-125M-dutch    |    176000000 | ğŸ‡³ğŸ‡±         |              512 |
| gpt2-medium-dutch     |    380000000 | ğŸ‡³ğŸ‡±         |              512 |
| gpt2-large-dutch      |    812000000 | ğŸ‡³ğŸ‡±         |              512 |
| Bor-1B                |   1190000000 | ğŸ‡³ğŸ‡±ğŸ‡¬ğŸ‡§       |             4096 |
| Llama-3.2-1B          |   1240000000 | ğŸŒ          |           128000 |
| gpt-neo-1.3B-dutch    |   1420000000 | ğŸ‡³ğŸ‡±         |              512 |
| Fietje-2              |   2700000000 | ğŸ‡³ğŸ‡±*        |             2048 |
| Phi-3.5-mini-instruct |   3800000000 | ğŸŒ          |           128000 |

### token_perplexity

| dataset       |   gpt-neo-125M-dutch |   gpt2-medium-dutch |   gpt2-large-dutch |   Bor-1B |   Llama-3.2-1B |   gpt-neo-1.3B-dutch |   Fietje-2 |   Phi-3.5-mini-instruct |
|:--------------|---------------------:|--------------------:|-------------------:|---------:|---------------:|---------------------:|-----------:|------------------------:|
| mc4_nl        |                23.2  |               16.62 |              18.27 |    15.1  |          13.33 |                17.93 |       4.62 |                   10.62 |
| culturax_nl   |                29.83 |               21.18 |              23.45 |    16.18 |          14.31 |                22.89 |       4.66 |                   11.24 |
| fineweb_edu_4 |                28.51 |               17.5  |              18.89 |    14.69 |          11    |                21.27 |      12.92 |                    6.86 |

### tokens_per_word

| dataset       |   gpt-neo-125M-dutch |   gpt2-medium-dutch |   gpt2-large-dutch |   Bor-1B |   Llama-3.2-1B |   gpt-neo-1.3B-dutch |   Fietje-2 |   Phi-3.5-mini-instruct |
|:--------------|---------------------:|--------------------:|-------------------:|---------:|---------------:|---------------------:|-----------:|------------------------:|
| mc4_nl        |                 1.28 |                1.28 |               1.28 |     1.37 |           1.76 |                 1.28 |       2.08 |                    1.84 |
| culturax_nl   |                 1.31 |                1.31 |               1.31 |     1.4  |           1.78 |                 1.31 |       2.11 |                    1.87 |
| fineweb_edu_4 |                 1.72 |                1.72 |               1.72 |     1.3  |           1.18 |                 1.72 |       1.16 |                    1.31 |

### cumulative_word_bits

| dataset       |   gpt-neo-125M-dutch |   gpt2-medium-dutch |   gpt2-large-dutch |   Bor-1B |   Llama-3.2-1B |   gpt-neo-1.3B-dutch |   Fietje-2 |   Phi-3.5-mini-instruct |
|:--------------|---------------------:|--------------------:|-------------------:|---------:|---------------:|---------------------:|-----------:|------------------------:|
| mc4_nl        |                 5.24 |                4.66 |               4.8  |     5.01 |           6.23 |                 4.76 |       4.31 |                    5.93 |
| culturax_nl   |                 6.04 |                5.39 |               5.56 |     5.28 |           6.46 |                 5.54 |       4.42 |                    6.18 |
| fineweb_edu_4 |                 8.18 |                6.97 |               7.14 |     4.89 |           3.95 |                 7.46 |       4.13 |                    3.48 |
"""
