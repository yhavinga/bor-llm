{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeb/Developer/yhavinga/bor-llm/venvtorchrocm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL = \"yhavinga/Bor-1B\"\n",
    "model_kwargs = {\n",
    "    \"device_map\": \"auto\",\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"use_flash_attention_2\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 330, 377, 2], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Be sure to add the eos token to the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yhavinga/dutch-llama-tokenizer\", add_eos_token=True)\n",
    "tokenizer(\"Aap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore language model evaluation metrics grounded in information theory. We'll focus on:\n",
    "1. Token-level perplexity = exp(-1/T ∑log P(token_i|token_{<i}, θ))\n",
    "2. Bits per Word (BPW) = -1/W ∑log₂ P(token_i|token_{<i}, θ)\n",
    "\n",
    "These metrics help us understand how well our model predicts text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeb/Developer/yhavinga/bor-llm/venvtorchrocm/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at ../aten/src/ATen/Context.cpp:296.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level conditional probabilities and log probabilities:\n",
      "▁Een         → P(token|context)=0.017578 (log P=-4.0411)\n",
      "▁grote       → P(token|context)=0.002365 (log P=-6.0469)\n",
      "▁zak         → P(token|context)=0.000362 (log P=-7.9228)\n",
      "▁be          → P(token|context)=0.000083 (log P=-9.3913)\n",
      "uken         → P(token|context)=0.233398 (log P=-1.4550)\n",
      "n            → P(token|context)=0.388672 (log P=-0.9450)\n",
      "oten         → P(token|context)=0.164062 (log P=-1.8075)\n",
      ".            → P(token|context)=0.022827 (log P=-3.7798)\n",
      "</s>         → P(token|context)=0.000122 (log P=-9.0148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeb/Developer/yhavinga/bor-llm/venvtorchrocm/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:456: UserWarning: Flash attention support on Navi31 GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:225.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/home/yeb/Developer/yhavinga/bor-llm/venvtorchrocm/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:456: UserWarning: Memory Efficient attention on Navi31 GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:269.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Een grote zak beukennoten.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "input_ids = inputs.input_ids[0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get conditional log probabilities for each token\n",
    "logits = outputs.logits[0]  # shape: [sequence_length, vocab_size]\n",
    "conditional_probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# conditional_probs[i] contains predictions for token i+1\n",
    "token_metrics = {}\n",
    "for i in range(len(tokens)-1):  # -1 because we don't predict after EOS\n",
    "    next_token_id = input_ids[i + 1]\n",
    "    conditional_prob = conditional_probs[i, next_token_id].item()\n",
    "    conditional_log_prob = torch.log(torch.tensor(conditional_prob)).item()\n",
    "    token_metrics[tokens[i+1]] = {\n",
    "        \"conditional_prob\": conditional_prob,\n",
    "        \"conditional_log_prob\": conditional_log_prob\n",
    "    }\n",
    "\n",
    "print(\"Token-level conditional probabilities and log probabilities:\")\n",
    "for token, metrics in token_metrics.items():\n",
    "    print(f\"{token:12} → P(token|context)={metrics['conditional_prob']:.6f} \"\n",
    "          f\"(log P={metrics['conditional_log_prob']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's quickly recap the core idea from Shannon's information theory. The information content of an event is inversely proportional to its probability. A highly probable event, like a fair coin landing heads, carries little information - just 1 bit. A rare event, like a specific word with a probability of 0.001, carries much more information, around 10 bits in this example. This is quantified using the negative base-2 logarithm of the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information content of a fair coin flip (heads): 1.00 bits\n",
      "Information content of a rare word (p=0.001): 9.97 bits\n"
     ]
    }
   ],
   "source": [
    "def information_content(probability):\n",
    "  \"\"\"Calculates the information content of an event.\n",
    "\n",
    "  Args:\n",
    "    probability: The probability of the event (0 < probability <= 1).\n",
    "\n",
    "  Returns:\n",
    "    The information content in bits.\n",
    "  \"\"\"\n",
    "  return -np.log2(probability)\n",
    "\n",
    "# Example\n",
    "probability_of_heads = 0.5\n",
    "ic_heads = information_content(probability_of_heads)\n",
    "print(f\"Information content of a fair coin flip (heads): {ic_heads:.2f} bits\")\n",
    "\n",
    "probability_of_rare_word = 0.001\n",
    "ic_rare_word = information_content(probability_of_rare_word)\n",
    "print(f\"Information content of a rare word (p=0.001): {ic_rare_word:.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For language models:\n",
    "- Each token prediction has an information content based on its conditional probability\n",
    "- BPW measures average information content per word\n",
    "- Lower BPW indicates better predictions (less \"surprise\" per word)\n",
    "\n",
    "To calculate BPW:\n",
    "1. Sum conditional log probabilities (in base e) to get the log likelihood - ∑log P(token_i|token_{<i}, θ)\n",
    "2. Convert to base 2 by dividing by log(2) \n",
    "3. Normalize by number of words\n",
    "\n",
    "This gives us the average number of bits needed per word according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Metrics:\n",
      "Token-level perplexity: 138.91\n",
      "Bits per word: 16.02\n"
     ]
    }
   ],
   "source": [
    "def calculate_sequence_metrics(text, token_metrics):\n",
    "    \"\"\"\n",
    "    Calculates sequence-level evaluation metrics.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (log_likelihood, num_tokens, log_likelihood_bits, num_words)\n",
    "    \"\"\"\n",
    "    # Sum conditional log probabilities for sequence log-likelihood\n",
    "    log_likelihood = sum(m['conditional_log_prob'] for m in token_metrics.values())\n",
    "    \n",
    "    # Convert to bits (log₂(P) = log_e(P)/log_e(2))\n",
    "    log_likelihood_bits = -log_likelihood / np.log(2.0)\n",
    "    \n",
    "    num_tokens = len(token_metrics)\n",
    "    num_words = len(text.split())\n",
    "    \n",
    "    return log_likelihood, num_tokens, log_likelihood_bits, num_words\n",
    "\n",
    "log_likelihood, num_tokens, log_likelihood_bits, num_words = calculate_sequence_metrics(\n",
    "    sentence, token_metrics\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "token_perplexity = np.exp(-log_likelihood / num_tokens)\n",
    "bits_per_word = log_likelihood_bits / num_words\n",
    "\n",
    "print(f\"Sequence Metrics:\")\n",
    "print(f\"Token-level perplexity: {token_perplexity:.2f}\")\n",
    "print(f\"Bits per word: {bits_per_word:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvtorchrocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
